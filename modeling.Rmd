---
title: "Modeling"
author: "Brian Stamper"
date: "November 29, 2017"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

For the modeling step at this time we will restrict our focus to the Ohio LMI and O\*Net data, with an eye toward incorporating other data sets in a future iteration of analysis. We will combine the O\*Net data on occupational Knowledge, Skills, Abilities, and Work Activities ("KSAW") and seek which variables most inform which occupations Ohio LMI project to have a high growth rate (> 10% over a ten-year projection). We emphasize that the goal is to create the mechanism which can identify these factors, rather than the predictive capacity of the model itself.

```{r prelims, warning = FALSE, message = FALSE}
# Package dependencies and data directories
library(data.table)
library(randomForest)
library(readxl)
library(ggplot2)

if(!dir.exists('data/onet')) dir.create('data/onet')
if(!dir.exists('data/ohiolmi')) dir.create('data/ohiolmi') # should already exist from manual data download
if(!dir.exists('data/processed')) dir.create('data/processed')
if(!dir.exists('out')) dir.create('out')
```

### Ohio Labor Market Information

This data must be downloaded through a web interface at http://ohiolmi.com/proj/OhioJobOutlook.htm. We select area "Ohio to 2024", report "All Ohio Tables", report format "Excel". The resulting file is saved as `data/ohiolmi/Ohio2024.xlsx`.

```{r ohiolmi-occ}

# Function to check the four-digit extension to the soc code is numeric and not all zero
is_soc6 <- function(x) {
  subsoc <- substr(x, 4, 7)
  grepl('[0-9]{4}', subsoc) & subsoc != '0000'
}

# The parameters to read_excel() were determined manually by opening the data in Excel
ohiolmi_occ <- setDT(read_excel(path = 'data/ohiolmi/Ohio2024.xlsx', 
                                sheet = 'Occupation Detail', 
                                skip = 5,
                                n_max = 756,
                                na = 'N/A'))

# Visual inspection of the data before and after the following statement is used to confirm
# the correct names are being applied to the correct columns.
names(ohiolmi_occ) <- c('soc_code_lmi',
                        'soc_desc_lmi',
                        'empl_now_lmi',
                        'occ_empl_projected_lmi',
                        'occ_empl_change_lmi',
                        'occ_empl_change_pct_lmi',
                        'ann_growth_lmi',
                        'ann_replace_lmi',
                        'ann_total_lmi',
                        'med_wage_lmi',
                        'note_lmi',
                        'ed_needed_lmi',
                        'exp_needed_lmi',
                        'otj_train_needed_lmi')

# Remove aggregation rows and keep only true SOC-6
ohiolmi_occ <- ohiolmi_occ[is_soc6(soc_code_lmi)]

save(ohiolmi_occ, file = 'data/processed/ohiolmi_occ.rds')
```

### O*Net KSAW data

```{r ksaw}
# Downloader function to save repetitive code and file transfers
onet_loader <- function(fn, ...) {
  if(!file.exists(paste0('data/onet/', fn))) {
    download.file(url = paste0('https://www.onetcenter.org/dl_files/database/db_22_1_text/', fn),
                  destfile = paste0('data/onet/', fn),
                  mode = 'wb')
  }
  fread(paste0('data/onet/', fn), na.strings = 'n/a', ...)
}

onet_ksaw <- rbindlist(list(onet_loader('Knowledge.txt'),
                            onet_loader('Skills.txt'),
                            onet_loader('Abilities.txt'),
                            onet_loader('Work%20Activities.txt')))


# Separate the six-digit SOC and filter for cases where the suffix == 00 (the aggregate)
onet_ksaw[, soc_code := substr(`O*NET-SOC Code`, 1, 7)]
onet_ksaw[, soc_suff := substr(`O*NET-SOC Code`, 9, 10)]
onet_ksaw <- onet_ksaw[soc_suff == '00']

# rename and select relevant variables
setnames(onet_ksaw,
         old = c('Element ID',
                 'Element Name',
                 'Scale ID',
                 'Data Value'),
         new = c('elem_id',
                 'elem_name',
                 'scale_id',
                 'value'))

onet_ksaw <- subset(onet_ksaw,
                         select = c('soc_code',
                                    'elem_id',
                                    'elem_name',
                                    'scale_id',
                                    'value'))

# Cast the 'importance' next to the 'level' for each soc/knowledge
onet_ksaw <- dcast(onet_ksaw, soc_code + elem_id + elem_name ~ scale_id, value.var = 'value')

# For each occupation, we want to know if a given knowledge area is
# in the upper quartile for importance (Scale ID = 'IM') *AND* 
# upper half for competency required (Scale ID = 'LV')
im_break <- .75
lv_break <- .5

onet_ksaw[, high_IM := quantile(IM, im_break), by = 'elem_id']
onet_ksaw[, high_LV := quantile(LV, lv_break), by = 'elem_id']

# Determine if both importance and level are above both for the occupation:
onet_ksaw[, high_IMLV := IM > high_IM & LV > high_LV]

# Cast the score wide over the knowledge areas:
onet_ksaw <- dcast(onet_ksaw, soc_code ~ elem_id, value.var = 'high_IMLV')

# Make the variable names syntactically valid
names(onet_ksaw) <- make.names(names(onet_ksaw))

# Save the processed KSAW data
save(onet_ksaw, file = 'data/processed/onet_ksaw.rds')


# Also get the content model lookup for later reference
onet_content_model <- onet_loader('Content%20Model%20Reference.txt')
names(onet_content_model) <- c('elem_id', 'elem_name', 'elem_desc')
# Match the elem_id to the variable names in onet_ksaw
onet_content_model$elem_id <- make.names(onet_content_model$elem_id)
save(onet_content_model, file = 'data/processed/onet_content_model.rds')
```

### Identify high growth occupations
We define a high growth occupation using the Ohio LMI Job Outlook data indicating
a projected employment growth of 10% over 10 years. We then attach this indicator
to the KSAW high IM/LV indicators.

```{r highgrowth}
# Define high growth
ohiolmi_occ[, high_growth := occ_empl_change_pct_lmi > .1]

# Known issue of not all SOCs match up, ignoring -- can't do much about it
ksaw_by_soc <- merge(onet_ksaw, 
                     subset(ohiolmi_occ, select = c('soc_code_lmi', 'high_growth')), 
                     by.x = 'soc_code', 
                     by.y = 'soc_code_lmi')

```


### Determine an appropriate information gain threshold
To identify which variables in the KSAW data are most indicative of whether an 
occupation will or will not fall into the high growth range we measure the 
information gain with respect to the high growth indicator. During the process
it was discovered that we get better performance from this model if we scale
the information gain measure as a proportion of the entropy of the parent node.

The following process is used to determine an ideal value for an information gain threshold for
this analysis. From prior experimentation it was determined that this threshold must
lie somewhere between 0 (which would allow all variables to be considered
informative) and .09 (which would only allow a few variables to pass). To
determine a good value we try each threshold value from 0 to .09,
in .01 steps. For each of these values we repeatedly sample sets of occupations,
each sample containing a balanced set of high-growth and non-high-growth 
occupations. Then, using those samples, we measure the information gain for
each variable and throw out any variable that falls below the threshold. With
the remaining variables we separate the sample into 80%/20% training and test sets.
A random forest classification tree is built using the training set and evaluated
against the test set. The results are compiled in a data.table, one row per sample.

```{r infogain}
# Entropy function for a single boolean variable, H(Y)
entropy <- function(y) {
  if(length(y) == 0) return(1)
  p1y <- sum(y) / length(y)
  p2y <- sum(!y) / length(y)
  if(p1y == 0 | p2y == 0) return(0)
  - p1y * log2(p1y) - p2y * log2(p2y)
}

# Conditional entropy function for two boolean variables, H(Y|X)
cond_entropy <- function(y, x) {
  stopifnot(length(x) == length(y))
  p1x <- sum(x) / length(x)
  p2x <- sum(!x) / length(x)
  p1x * entropy(y[x]) + p2x * entropy(y[!x])
}

# Information gained about boolean y by boolean x -- that is, H(Y) - H(Y|X)
# Here the formula has been modified to measure info_gain relative to the parent entropy.
info_gain <- function(y, x) {
  stopifnot(length(x) == length(y))
  (entropy(y) - cond_entropy(y, x)) / entropy(y)
}

# Set seed for reproducibility
set.seed(0)

# Build a data table with each row being the results of one sample tested,
# with 100 samples for each threshold 0:9 * .01
find_ig_thresh <- rbindlist(lapply(0:9 * .01, function(ig_thresh) {
  rbindlist(lapply(1:100, function(iteration) {
    
    # Prior exploration indicate we have at least 120 in each of high-growth and non-high-growth,
    # so we can create a reasonably balanced sample using near that amount in each subset.
    # (Minimum entropy of the response variable at these settings is 0.971)
    ksaw_by_soc_sample <- rbind(
      ksaw_by_soc[(high_growth)][sample(.N, sample(100:120, 1))],
      ksaw_by_soc[!(high_growth)][sample(.N, sample(100:120, 1))]
    )
    
    # Measure the information gain of each variable according to this sample
    information_gain <- sapply(ksaw_by_soc_sample[, -c('soc_code', 'high_growth')], function(this_value) {
      info_gain(ksaw_by_soc_sample$high_growth, this_value)
    })
    
    # Convert that vector into a data.table
    information_gain <- data.table(elem_id = names(information_gain),
                                   elem_ig = information_gain)
    
    # Identify which variables are high info at this threshold level
    high_info_vars <- information_gain[elem_ig > ig_thresh]$elem_id
    
    # Prepare train/test sets from a subset with only the high info variables
    ksaw_by_soc_sample_limited <- subset(ksaw_by_soc_sample, select = c('high_growth', high_info_vars))
    
    train <- sample(c(TRUE, FALSE), nrow(ksaw_by_soc_sample_limited), replace = TRUE, prob = c(.8, .2))
    test <- !train
    
    ksaw_train <- ksaw_by_soc_sample_limited[train]
    ksaw_test <- ksaw_by_soc_sample_limited[test]
    
    # Prevent an error caused by having zero high info variables at a given threshold/sample
    if(length(high_info_vars) > 0) {
      ktree <- randomForest(as.factor(high_growth) ~ ., ksaw_train, importance = TRUE, ntree = 1000) 
      test_pred <- predict(ktree, ksaw_test)
    } else {
      test_pred <- !ksaw_test$high_growth  # If zero vars, define the prediction as exactly wrong
    }
    
    # Construct a single-row table summarizing this iteration
    data.table(ig_thresh,
               iteration,
               sample_size = nrow(ksaw_by_soc_sample),
               qom = sum(test_pred == ksaw_test$high_growth) / nrow(ksaw_test), 
               nvars = length(high_info_vars))
  }))
}))
```

### Interpreting the results
Looking at the mean quality of match (qom) by information gain threshold, we observe that there
is little variation, but also that we can 
acheive a high quality of match, decent stability, and a reasonably interpretable low 
number of variables if we choose an information gain threshold such that we are only considering
less than 20 of the original variables.
```{r interpret}
find_ig_thresh[, .(qom_mean = mean(qom),
                   qom_sd = sd(qom),
                   nvars_mean = median(nvars)),
               by = ig_thresh][order(ig_thresh)]

ggplot(find_ig_thresh, aes(x = as.factor(ig_thresh), y = qom)) +
  geom_boxplot() +
  xlab('Information gain threshold') +
  ylab('Quality of match')

ggplot(find_ig_thresh, aes(x = as.factor(ig_thresh), y = nvars)) +
  geom_boxplot() +
  xlab('Information gain threshold') +
  ylab('Number of variables identified')

```

### Identify informative variables
Now using this threshold, we reevaluate the data to seek the most informative variables.

```{r informative}
ig_thresh <- 0.04

# Measure the information gain of each variable according to the entire dataset
information_gain <- sapply(ksaw_by_soc[, -c('soc_code', 'high_growth')], function(this_value) {
  info_gain(ksaw_by_soc$high_growth, this_value)
})

information_gain <- data.table(elem_id = names(information_gain),
                               elem_ig = information_gain)

information_gain <- merge(information_gain[elem_ig > ig_thresh], onet_content_model, by = 'elem_id')
information_gain[order(-elem_ig), -c('elem_desc')]
```

